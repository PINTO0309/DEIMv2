__include__: [
  '../dataset/wholebody34.yml',
  '../runtime.yml',
  '../base/dataloader.yml',
  '../base/optimizer.yml',
  '../base/deimv2.yml',
]

output_dir: ./outputs/deimv2_dinov3_s_wholebody34ft

DEIM:
  backbone: DINOv3STAs

DINOv3STAs:
  name: vit_tiny
  embed_dim: 192
  weights_path: ./ckpts/vitt_distill.pt
  interaction_indexes: [5,8,11]   # only need the [1/8, 1/16, 1/32]
  num_heads: 3

HybridEncoder:
  in_channels: [192, 192, 192]
  depth_mult: 0.67
  expansion: 0.34
  hidden_dim: 192
  dim_feedforward: 512

DEIMTransformer:
  feat_channels: [192, 192, 192]
  hidden_dim: 192
  dim_feedforward: 512
  num_layers: 4  # 4 5 6
  eval_idx: -1  # -2 -3 -4
  num_queries: 1750

PostProcessor:
  num_top_queries: 1750

## Optimizer
optimizer:
  type: AdamW

  params:
    -
      # except norm/bn/bias in self.dinov3
      params: '^(?=.*.dinov3)(?!.*(?:norm|bn|bias)).*$'
      lr: 0.000007812
    -
      # including all norm/bn/bias in self.dinov3
      params: '^(?=.*.dinov3)(?=.*(?:norm|bn|bias)).*$'
      lr: 0.000007812
      weight_decay: 0.
    -
      # including all norm/bn/bias except for the self.dinov3
      params: '^(?=.*(?:sta|encoder|decoder))(?=.*(?:norm|bn|bias)).*$'
      weight_decay: 0.

  lr: 0.00003125 # 0.0005 / 4 * GPUs
  betas: [0.9, 0.999]
  weight_decay: 0.00000625 # 0.0001 / 4 * GPUs

# Increase to search for the optimal ema
epoches: 28 # 120 + 4n

## Our LR-Scheduler
flat_epoch: 14    # 4 + epoch // 2, e.g., 40 = 4 + 72 / 2
no_aug_epoch: 8

## Our DataAug
train_dataloader:
  dataset:
    transforms:
      ops:
        # - {type: Mosaic, output_size: 320, rotation_range: 10, translation_range: [0.1, 0.1], scaling_range: [0.5, 1.5],
        #    probability: 1.0, fill_value: 0, use_cache: True, max_cached_images: 50, random_pop: True}
        - {type: RandomPhotometricDistort, p: 0.5}
        - {type: RandomZoomOut, p: 0.5, fill: 0, side_range: [1.0, 1.5]}
        - {type: RandomIoUCrop, p: 0.8}
        - {type: SanitizeBoundingBoxes, min_size: 1}
        # right-front <-> left-front
        # right-side <-> left-side
        # right-back <-> left-back
        # hand_left <-> hand_right
        - {type: RandomHorizontalFlipWithClass, class_pairs: [[9, 15], [10, 14], [11, 13], [27, 28]]}
        - {type: Resize, size: [640, 640], }
        - {type: SanitizeBoundingBoxes, min_size: 1}
        - {type: ConvertPILImage, dtype: 'float32', scale: True}
        - {type: Normalize, mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225]}
        - {type: ConvertBoxes, fmt: 'cxcywh', normalize: True}
      policy:
        # The policy block configures the staged augmentation scheduler used by engine/data/transforms/container.py:57. The loader wraps your ops in Compose, and policy['name'] selects which scheduling routine to run (default, stop_epoch, or stop_sample).
        # In DEIM configs we rely on stop_epoch (see engine/data/transforms/container.py:77). The dataloader calls set_epoch, so every batch knows the current epoch and the policy can enable/disable transforms dynamically.
        # Fields for stop_epoch
        # ops: list of transform class names that the scheduler is allowed to turn on/off (e.g. ['Mosaic', 'RandomPhotometricDistort', ...]). Names must match the actual class names printed when transforms are created.
        # epoch: either
        # a single integer → once dataset.epoch >= epoch, transforms listed in ops are skipped; or
        # a list with three values [no_aug_end, mosaic_end, full_stop] → implements a four-stage schedule (engine/data/transforms/container.py:90).
        # epoch < no_aug_end: skip all listed ops (pure warmup).
        # no_aug_end ≤ epoch < mosaic_end: apply listed ops, but Mosaic only fires with mosaic_prob and RandomZoomOut / RandomIoUCrop are suppressed when Mosaic already ran.
        # mosaic_end ≤ epoch < full_stop: listed ops run every time (no Mosaic special-casing).
        # epoch ≥ full_stop: listed ops are skipped again for the final no‑augmentation phase.
        epoch: [4, 14, 20]   # list
        ops: ['Mosaic', 'RandomPhotometricDistort', 'RandomZoomOut', 'RandomIoUCrop']
      mosaic_prob: -0.1
  total_batch_size: 3 # total batch size equals to 32 (4 GPUs * 8 batch)
  num_workers: 3

  collate_fn:
    base_size: 640
    mixup_prob: 0.0
    ema_restart_decay: 0.9999
    base_size_repeat: 20
    mixup_epochs: [4, 14]
    stop_epoch: 20
    copyblend_prob: 0.0
    copyblend_epochs: [4, 20]

val_dataloader:
  dataset:
    transforms:
      ops:
        - {type: Resize, size: [640, 640], }
        - {type: ConvertPILImage, dtype: 'float32', scale: True}
        - {type: Normalize, mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225]}
  total_batch_size: 16 # total batch size equals to 64 (4 GPUs * 16 batch)
  num_workers: 16

DEIMCriterion:
  matcher:
    # change matcher
    change_matcher: True
    iou_order_alpha: 4.0
    matcher_change_epoch: 18 # ≈ 0.9 * (epoches - no_aug_epoch)
